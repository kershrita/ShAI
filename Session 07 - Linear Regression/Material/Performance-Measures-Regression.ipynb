{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Performance-Measures-Regression.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"charming-christian"},"source":["# Performance Measures"],"id":"charming-christian"},{"cell_type":"markdown","metadata":{"id":"jewish-gibson"},"source":["We have to define some metrics to evaluate our model. Those metrics are different for regression and classification.\n","\n","When we are dealing with a regression task, there are plenty of metrics that we can use to evaluate the model. We are going to talk about three metrics that you are going to be dealing with.\n","\n","**MAE: Mean Absolute Error**\n","\n","**MSE: Mean Squared Error**\n","\n","**RMSE: Root Mean Squared Error**\n","\n","--------------------------------------------"],"id":"jewish-gibson"},{"cell_type":"code","metadata":{"id":"2WdmvhI-yW9J"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"2WdmvhI-yW9J","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"stuffed-sperm"},"source":["![img_01.png](attachment:img_01.png)"],"id":"stuffed-sperm"},{"cell_type":"markdown","metadata":{"id":"metallic-administrator"},"source":["# MAE"],"id":"metallic-administrator"},{"cell_type":"markdown","metadata":{"id":"significant-deployment"},"source":["**Simply put, the average difference observed in the predicted and actual values across the whole test set.**\n","\n","the algorithm takes the differences in all of the predicted and actual prices, adds them up and then divides them by the number of observations. It doesn’t matter if the prediction is higher or lower than the actual price, the algorithm just looks at the absolute value. **A lower value indicates better accuracy.**"],"id":"significant-deployment"},{"cell_type":"markdown","metadata":{"id":"dietary-surgeon"},"source":["![image017.png](attachment:image017.png)"],"id":"dietary-surgeon"},{"cell_type":"markdown","metadata":{"id":"unavailable-strap"},"source":["**Keep in mind that the MAE treats all errors equally, while other metrics like the MSE give more weight for larger errors**"],"id":"unavailable-strap"},{"cell_type":"markdown","metadata":{"id":"fifteen-concert"},"source":["##### Using NumPy"],"id":"fifteen-concert"},{"cell_type":"code","metadata":{"id":"necessary-finger","outputId":"4ff7be0f-3fc3-4972-d312-86ff99493dff"},"source":["# We are going to define some arrays as the true values and as our predictions\n","y_true = [40, 20, 30, 20, 25, 15, 4, 77, 60, 93, 56, 44, 23, 5]\n","y_predicted = [37, 21, 29, 19, 25, 20, 2, 50, 57, 73, 49, 41, 23, 8]\n","\n","\n","import numpy as np\n","\n","y_true = np.array(y_true)  # Convert Python list to numpy array so we can perform element-wise operations\n","y_predicted = np.array(y_predicted)\n","\n","\n","# Take your time to understand the code here, it's just an implementation of the equation above\n","mae = (np.sum(np.abs(y_true - y_predicted))) / len(y_true)  \n","\n","print(f'Mean Absolute Erroe MAE = {mae}')"],"id":"necessary-finger","execution_count":null,"outputs":[{"output_type":"stream","text":["Mean Absolute Erroe MAE = 5.428571428571429\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"married-headline"},"source":["##### fortunately, we don't have to do this every time. Scikit-learn has functions that can do this directly"],"id":"married-headline"},{"cell_type":"code","metadata":{"id":"novel-costume","outputId":"ae5cc2b4-08b5-4aea-ef47-afc6a34c8e5f"},"source":["from sklearn.metrics import mean_absolute_error\n","\n","mae = mean_absolute_error(y_true, y_predicted)\n","\n","\n","print(f'Mean Absolute Error MAE = {mae}')"],"id":"novel-costume","execution_count":null,"outputs":[{"output_type":"stream","text":["Mean Absolute Error MAE = 5.428571428571429\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"impressed-skating"},"source":["## MSE"],"id":"impressed-skating"},{"cell_type":"markdown","metadata":{"id":"unusual-record"},"source":["**squares the errors so a difference of 2, becomes 4, a difference of 3 becomes 9**\n","\n","As you can see, as a result of the squaring, it assigns more weight to the bigger errors. The algorithm then continues to add them up and average them.\n","**As before, lower the number the better.**"],"id":"unusual-record"},{"cell_type":"markdown","metadata":{"id":"fleet-simple"},"source":["![image003-1.png](attachment:image003-1.png)"],"id":"fleet-simple"},{"cell_type":"markdown","metadata":{"id":"widespread-divide"},"source":["**One disadvantage for this metric is that it is not the same in units as our dependent value (the target feature). So MSE is hard to interpret and be compared to other metrics.**"],"id":"widespread-divide"},{"cell_type":"code","metadata":{"id":"optimum-belgium","outputId":"c2ac0c1a-c254-4081-fc45-64db85fe39a8"},"source":["from sklearn.metrics import mean_squared_error\n","\n","mse = mean_squared_error(y_true, y_predicted)\n","\n","# Remember, this value is in squared units and that's why it is much greater than the MAE\n","print(f'Mean Squared Error MSE = {mse}')"],"id":"optimum-belgium","execution_count":null,"outputs":[{"output_type":"stream","text":["Mean Squared Error MSE = 89.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"green-sandwich"},"source":["## RMSE"],"id":"green-sandwich"},{"cell_type":"markdown","metadata":{"id":"educational-culture"},"source":["**RMSE can be obtained just be obtaining the square root of MSE**\n","\n","This number is in the same unit as the value that was to be predicted."],"id":"educational-culture"},{"cell_type":"markdown","metadata":{"id":"inappropriate-reporter"},"source":["![13.-RMSE-formula.png](attachment:13.-RMSE-formula.png)"],"id":"inappropriate-reporter"},{"cell_type":"markdown","metadata":{"id":"interracial-universal"},"source":["##### There is no scikit-learn function that computes the RMSE directly, but we can find it easily by taking the square root of the MSE:"],"id":"interracial-universal"},{"cell_type":"code","metadata":{"id":"juvenile-raise","outputId":"53e414cf-af56-4034-8da6-5891c9566c78"},"source":["rmse = np.sqrt(mse)\n","\n","print(f'Root Mean Squared Error RMSE = {rmse}')"],"id":"juvenile-raise","execution_count":null,"outputs":[{"output_type":"stream","text":["Root Mean Squared Error RMSE = 9.433981132056603\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"soviet-oregon"},"source":["## What is the best metric to use?"],"id":"soviet-oregon"},{"cell_type":"markdown","metadata":{"id":"collected-suffering"},"source":["It is entirely your call.\n","\n","\n","MAE is just a simple way to find out how much your predictions are different from the actual true values"],"id":"collected-suffering"},{"cell_type":"markdown","metadata":{"id":"divided-smart"},"source":["MSE & RMSE are really useful when you want to see if the outliers are messing with your predictions.\n","\n","It’s possible that you might decide to investigate those outliers and remove them altogether from your dataset. It's useful to use MSE or RMSE here.\n","\n","But since MSE has different units, it's usually better to use RMSE "],"id":"divided-smart"},{"cell_type":"markdown","metadata":{"id":"clinical-aviation"},"source":["## How to decide if a certain value for MAE or RMSE is good?"],"id":"clinical-aviation"},{"cell_type":"markdown","metadata":{"id":"becoming-commerce"},"source":["**To compare, we can go back and find the mean for our target column**\n","\n","Suppose that we have an RMSE of 200, and the average value (mean) of our target column is 8000. This means that we have an error rate of about 2.5%, which seems pretty good."],"id":"becoming-commerce"},{"cell_type":"markdown","metadata":{"id":"confirmed-panel"},"source":["# Good Luck"],"id":"confirmed-panel"}]}